{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practical'></a>\n",
    "\n",
    "## Using Requests + Beautiful Soup to extract information from a webpage.\n",
    "\n",
    "---\n",
    "\n",
    "Beautiful Soup is a python library useful for pulling data out of HTML and XML files.  It works with many parsers, such as XPath and can be executed in an IDE, so it can be much easier to work with when first extracting information from html.\n",
    "\n",
    "Please make sure that the required packages are installed: \n",
    "\n",
    "```bash\n",
    "# beautiful soup:\n",
    "> conda install bs4 \n",
    "> conda install lxml\n",
    "\n",
    "# or if conda doesn't work\n",
    "> pip install bs4\n",
    "> pip install lxml\n",
    "```\n",
    "\n",
    "Lets find another posting for a sweet set of wheels on Craigslist (You will probably ave to update the URL to one that hasn't expired.):\n",
    "\n",
    "![](assets/craigslist.jpg)\n",
    "\n",
    "> *Note: you will need to update this to a current/working craigslist post.*\n",
    "\n",
    "https://washingtondc.craigslist.org/doc/cto/6178337288.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "### Step 1: fetch the content by URL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code:  200\n",
      "\n",
      "First part of HTML document fetched as string:\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html class=\"no-js\">\n",
      "<head>\n",
      "<title>1983 Mercedes 380SL - cars &amp; trucks - by owner - vehicle automotive sale</title>\n",
      "    \t<link rel=\"canonical\" href=\"http://washingtondc.craigslist.org/doc/cto/6178337288.html\">\n",
      "\t<meta name=\"description\" content=\"A true Mercedes Benz classic that will only go up in value... This beautiful Grey 1983 well cared for Mercedes has a 3.8L V8 engine, 4 speed Automatic Transmission, PS, PB. Replaced original grey...\">\n",
      "\t<meta name=\"robots\" content=\"noar\n"
     ]
    }
   ],
   "source": [
    "# you will need the requests library in order to fully utilize bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# target web page\n",
    "url = \"https://washingtondc.craigslist.org/doc/cto/6178337288.html\"\n",
    "# establishing the connection to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# You can use status codes to understand how the target server responds to your request.\n",
    "#Ex. 200 = OK, 400 = Bad Request, 403 = Forbidden, 404 = Not Found\n",
    "print 'Status Code: ',response.status_code\n",
    "\n",
    "# Pull HTML string out of requests and convert to a python string\n",
    "html = response.text\n",
    "\n",
    "# The first 500 characters of the content\n",
    "print \"\\nFirst part of HTML document fetched as string:\\n\"\n",
    "print html[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[More information on request status codes](http://www.restapitutorial.com/httpstatuscodes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "### Step 2: Parse HTML document with Beautiful Soup\n",
    "\n",
    "This step allows us to access the elements of the document by XPATH expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soup queries are more like accessing information within a python object.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** There are many ways to get the elements in a \"soup\" object\n",
    "\n",
    "Here are a few ways to select HMTL elements as \"objects\" within \"soup\" as a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>1983 Mercedes 380SL - cars &amp; trucks - by owner - vehicle automotive sale</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Singular element\n",
    "soup.html.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1983 Mercedes 380SL - cars & trucks - by owner - vehicle automotive sale\n"
     ]
    }
   ],
   "source": [
    "# Just the text between elements\n",
    "print soup.html.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'CL'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find single or multiple elements\n",
    "# First parameter\n",
    "element = soup.findAll(\"a\", {\"class\": \"header-logo\"})\n",
    "element[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'$18500'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_search = soup.findAll('span', {\"class\": \"price\"})\n",
    "price_search[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# switching back to SF apartment listings\n",
    "response = requests.get(\"http://sfbay.craigslist.org/search/sfc/apa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "search_titles = soup.findAll(\"a\", {\"class\": \"hdrlnk\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data-id': '6178462035', 'href': '/sfc/apa/6178462035.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6178462043', 'href': '/sfc/apa/6178462043.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6178461647', 'href': '/sfc/apa/6178461647.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6178461105', 'href': '/sfc/apa/6178461105.html', 'class': ['result-title', 'hdrlnk']}\n",
      "{'data-id': '6178460712', 'href': '/sfc/apa/6178460712.html', 'class': ['result-title', 'hdrlnk']}\n"
     ]
    }
   ],
   "source": [
    "for link in search_titles[0:5]:\n",
    "    print link.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### > **Check:** How do we know which parameters `findAll()` takes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practice'></a>\n",
    "\n",
    "### Practice: can you select the price of our junker?  \n",
    "\n",
    " - Use XPath Helper to get an idea of where the element is within the HTML document.\n",
    " - Try to select using the soup.html.body.something.something method.\n",
    " - Try using findAll() to find a concise element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrapy'></a>\n",
    "<a scrapy-spiders></a>\n",
    "## What is [Scrapy](http://scrapy.org/)?\n",
    "\n",
    "---\n",
    "\n",
    "> *\"Scrapy is an application framework for writing web spiders that crawl web sites and extract data from them.\"*\n",
    "\n",
    "Below we will walkthrough the creation of a **spider** using scrapy. Spiders are automated processes that will crawl through a webpage or webpages and collect information.\n",
    "\n",
    "> **Note:** This code should be written in a script outside of jupyter notebook.\n",
    "\n",
    "<a id='scrapy-project'></a>\n",
    "### 1. Create a new Scrapy project\n",
    "\n",
    "In your terminal. `cd` into a directory you want to create your Crawler's folder.  I recommend the desktop for ease of access to the files inside we will need to edit.\n",
    "> `scrapy startproject craigslist`\n",
    "\n",
    "**Should create output that looks like this:**\n",
    "<blockquote>\n",
    "```\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Overridden settings: {}\n",
    "New Scrapy project 'craigslist' created in:\n",
    "    /Users/davidyerrington/virtualenvs/data/scraping/craigslist\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd craigslist\n",
    "    scrapy genspider example example.com\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "**That command generates a set of project files:**\n",
    "<blockquote>\n",
    "```\n",
    "craigslist/\n",
    "    scrapy.cfg\n",
    "    craigslist/\n",
    "        __init__.py\n",
    "        items.py\n",
    "        pipelines.py\n",
    "        settings.py\n",
    "        spiders/\n",
    "            __init__.py\n",
    "            ...\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "Generally, these are our files.  We will go into more detail on these soon.\n",
    "\n",
    " * **`scrapy.cfg`:** the project configuration file\n",
    " * **`craigslist/`:** the project’s python module, you’ll later import your code from here.\n",
    " * **`craigslist/items.py`:** the project’s items file.\n",
    " * **`craigslist/pipelines.py`:** the project’s pipelines file.\n",
    " * **`craigslist/settings.py`:** the project’s settings file.\n",
    " * **`craigslist/spiders/`:** a directory where you’ll later put your spiders.\n",
    " \n",
    "Long story, but please add this line to your craigslist/settings.py file before continuing:\n",
    " \n",
    " <blockquote>\n",
    " ```\n",
    " DOWNLOAD_HANDLERS = {'s3': None,}\n",
    " ```\n",
    " </blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='define-item'></a>\n",
    "### 2. Define an \"item\"\n",
    "\n",
    "Basically, when we define an item, it's telling our new application what it will be collecting.  In essence, an \"item\", is an entity that has attributes (ie: \"title\", \"description\", \"price\", etc) that are descriptive and relate to elements on pages that we will be scraping.  \n",
    "\n",
    "In more precise terms, this is a model (for those who are familliar with ORM or relational database terms).  Don't worry if this is a foreign concept.  The main idea to understand is that a model has attributes that closely resemble / relate to elements on our target web page(s).\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class CraigslistItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='spider-crawl'></a>\n",
    "### 3. A spider that crawls\n",
    "\n",
    "An item is a model that resembles data on a webpage.  A spider is something that crawls pages and uses our item model to to get and hold items for us.\n",
    "\n",
    "**Scrapy spiders are python classes.  Let's write our first file, called `craigslist_spider.py` and put it in our `/spiders` directory:**\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class CraigslistSpider(scrapy.Spider):\n",
    "    name = \"craigslist\"\n",
    "    allowed_domains = [\"craigslist.org\"]\n",
    "    start_urls = [\n",
    "        \"http://sfbay.craigslist.org/search/sfc/apa\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        filename = response.url.split(\"/\")[-2]\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "```\n",
    "\n",
    "**Next, let's dive in and crawl from our `/craigslist/craigslist` directory:**\n",
    "\n",
    "```\n",
    "> scrapy crawl craigslist\n",
    "```\n",
    "\n",
    "**What just happened?**\n",
    " * Our application requested the URLs from the `start_urls` class attribute.\n",
    " * Ran parse over the content containing the HTML markup, of each request URL.\n",
    " * What else?\n",
    " \n",
    "```python\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.body)\n",
    "```\n",
    "\n",
    "It saved a file in our base project directory.  It should be named based on the end of the URL.  In our case, it should create a file called \"sfc\".  This is taken directly from the Scrapy docs and it's only point is to illustrate the workflow so far.  It is kind of nice to have a reference to our HTML file though.  \n",
    "\n",
    "There might be some errors listed when we crawl, but they are fine for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='xpath-spider'></a>\n",
    "### 4. XPath + parsing with our spider\n",
    "\n",
    "So far, we've defined what fields we'll get, some urls to fetch, and saved some content to a file.  Let's actually do something interesting.\n",
    "\n",
    "**We should let our spider know about the item model we made earlier.  In the head of the `craigslist/craigslist/spiders/craigslist_spider.py`, lets add a new import:**\n",
    "\n",
    "```python\n",
    "from craigslist.items import CraigslistItem\n",
    "```\n",
    "\n",
    "> **Check:** Why won't it work otherwise?\n",
    "\n",
    "<br><br><br>\n",
    "**Let's replace our parse method, to find some data from our Craigslist spider response, and map it to our item model, CraigslistItem:**\n",
    "\n",
    "\n",
    "```python\n",
    "def parse(self, response): # define parse function \n",
    "    items = [] # element for storing scraped info\n",
    "\thxs = Selector(response) # selector is a function that allows us to grab html from the response(target website)\n",
    "\tfor sel in hxs.xpath(\"//li[@class='result-row']/p\"): # as we're using xpath languange we need to specify\n",
    "                        # that the paragraphs we are trying to isolate are expressed via xpath\n",
    "\t\titem = CraigslistItem()\n",
    "        item['title'] =  sel.xpath(\"a/text()\").extract() #title text from the 'a' element \n",
    "\t\titem['link']  =  sel.xpath(\"a/@href\").extract() # href/url from the 'a' element \n",
    "\t\titem['price'] =  sel.xpath('span/span[@class=\"result-price\"]/text()').extract()[0]\n",
    "                # price from the result price class nested in a few span elements.\n",
    "        items.append(item)\n",
    "\treturn items # shows scraped information as terminal output\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='save-examine'></a>\n",
    "### Save and examine our scraped data\n",
    "\n",
    "By default, we can save our crawled data as csv.  To save our data, we just need to pass a few optional parameters to our crawl call:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "> scrapy crawl craigslist -o items.csv -t csv\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "It's always good to iteratively check our data when developing a spider to make sure it's close to what we want. \n",
    "\n",
    "> *Pro tip:  The longer your iterations are between checks, the harder it's going to be to understand what's not working and fix bugs.*\n",
    "\n",
    "You should now have a file called '`items.csv`' in the directory you ran the `scrapy crawl` command from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='addendum'></a>\n",
    "## Addendum: leveraging XPath to get more results\n",
    "\n",
    "---\n",
    "\n",
    "Generally, a workflow that is useful in this context is to load the page in your Chrome browser, check out the page using the XPath Helper plugin, and from that derive your own XPath expressions based on the output.\n",
    "\n",
    "`text()` selects only the text of a given element (between the tags), and `@attribute_name` is used to select attributes.\n",
    "\n",
    "**Here are a few examples of `text()`**:\n",
    "<blockquote>\n",
    "```\n",
    "<h1>Darwin - The Evolution Of An Exhibition</h1>\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "The XPath selector for this:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "//h1/text()\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "**Here are a few examples of attributes**:\n",
    "\n",
    "And the description is contained inside a `<div>` tag with `id=\"description\"`:\n",
    "<blockquote>\n",
    "```\n",
    "<h2>Description:</h2>\n",
    "\n",
    "<div id=\"description\">\n",
    "Short documentary made for Plymouth City Museum and Art Gallery regarding the setup of an exhibit about Charles Darwin in conjunction with the 200th anniversary of his birth.\n",
    "</div>\n",
    "...\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "XPath\n",
    "<blockquote>\n",
    "```\n",
    "//div[@id='description']\n",
    "```\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='follow-links'></a>\n",
    "### Following links for more results\n",
    "\n",
    "100 results is pretty cool but what if we want more?  We need to follow the \"next\" links, and find new pages to grab.  Using the **`parse()`** method of our spider class, we only need to return another type of object.\n",
    "\n",
    "```python\n",
    "def parse(self, response):  \n",
    "\n",
    "    items = [] \n",
    "\thxs = Selector(response) \n",
    "    titles = hxs.xpath(\"//li[@class='result-row']/p\")\n",
    "    \n",
    "\tfor sel in titles:                     \n",
    "\t\titem = CraigslistItem()\n",
    "        item['title'] =  sel.xpath(\"a/text()\").extract() \n",
    "\t\titem['link']  =  sel.xpath(\"a/@href\").extract() \n",
    "\t\titem['price'] =  sel.xpath('span/span[@class=\"result-price\"]/text()').extract()[0]     \n",
    "        items.append(item)\n",
    "\treturn items \n",
    "\n",
    "    # Does the next page exist?  Let's get it!\n",
    "    next_page   = response.xpath(\"(//a[@class='button next']/@href)[1]\")\n",
    "\n",
    "    if next_page:\n",
    "        url = response.urljoin(next_page[0].extract())\n",
    "        yield scrapy.Request(url, self.parse)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
